{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jidza1972/Procurement-Anomaly-Dashboard/blob/main/Public_Procurement_Anomaly_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- App Configuration ---\n",
        "st.set_page_config(\n",
        "    page_title=\"Procurement Anomaly Detector\",\n",
        "    page_icon=\"üîé\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\",\n",
        ")\n",
        "\n",
        "# Custom CSS for a better look and feel\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .reportview-container {\n",
        "        background: #f0f2f6;\n",
        "    }\n",
        "    .sidebar .sidebar-content {\n",
        "        background: #ffffff;\n",
        "    }\n",
        "    .stButton>button {\n",
        "        color: #ffffff;\n",
        "        background-color: #0068c9;\n",
        "        border-radius: 8px;\n",
        "        border: none;\n",
        "        padding: 10px 20px;\n",
        "    }\n",
        "    .stButton>button:hover {\n",
        "        background-color: #00509e;\n",
        "        color: #ffffff;\n",
        "    }\n",
        "    .st-expander {\n",
        "        border: 1px solid #e6e9ef;\n",
        "        border-radius: 8px;\n",
        "    }\n",
        "    h1, h2, h3 {\n",
        "        color: #1e293b;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def generate_synthetic_data():\n",
        "    \"\"\"\n",
        "    Generates a synthetic procurement dataset with known anomalies.\n",
        "    This helps in demonstrating the app's functionality without real data.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    num_records = 500\n",
        "    num_anomalies = 25\n",
        "\n",
        "    # Normal data\n",
        "    data = {\n",
        "        'supplier_id': np.random.choice([f'SUP-{i:03}' for i in range(20)], num_records),\n",
        "        'item_category': np.random.choice(['Office Supplies', 'IT Hardware', 'Consulting', 'Construction'], num_records, p=[0.4, 0.3, 0.2, 0.1]),\n",
        "        'quantity': np.random.randint(1, 100, size=num_records),\n",
        "        'unit_price': np.random.uniform(10, 500, size=num_records),\n",
        "        'contract_duration_days': np.random.randint(30, 365, size=num_records)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df['total_cost'] = df['quantity'] * df['unit_price']\n",
        "\n",
        "    # --- Inject Anomalies ---\n",
        "    anomaly_indices = np.random.choice(df.index, num_anomalies, replace=False)\n",
        "    df['ground_truth_anomaly'] = 0\n",
        "    df.loc[anomaly_indices, 'ground_truth_anomaly'] = 1 # 1 for anomaly, 0 for normal\n",
        "\n",
        "    # Anomaly Type 1: Extremely high unit price\n",
        "    df.loc[anomaly_indices[:8], 'unit_price'] *= np.random.uniform(10, 20, size=8)\n",
        "\n",
        "    # Anomaly Type 2: Unusually large quantity\n",
        "    df.loc[anomaly_indices[8:15], 'quantity'] *= np.random.randint(15, 30, size=7)\n",
        "\n",
        "    # Anomaly Type 3: Suspiciously low contract duration for high cost\n",
        "    high_cost_anomalies = anomaly_indices[15:20]\n",
        "    df.loc[high_cost_anomalies, 'total_cost'] *= np.random.uniform(5, 10, size=5)\n",
        "    df.loc[high_cost_anomalies, 'contract_duration_days'] = np.random.randint(1, 7, size=5)\n",
        "\n",
        "    # Anomaly Type 4: Mismatched total cost\n",
        "    mismatch_indices = anomaly_indices[20:]\n",
        "    df.loc[mismatch_indices, 'total_cost'] *= np.random.uniform(2, 5, size=len(mismatch_indices))\n",
        "\n",
        "    # Recalculate total cost for non-mismatched anomalies to ensure consistency\n",
        "    df['total_cost'] = df['quantity'] * df['unit_price']\n",
        "    df.loc[mismatch_indices, 'total_cost'] *= 1.5 # Keep the mismatch for these specific anomalies\n",
        "\n",
        "    return df.sample(frac=1).reset_index(drop=True) # Shuffle data\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Creates a preprocessing pipeline to handle categorical and numerical features.\n",
        "    \"\"\"\n",
        "    # Identify categorical and numerical features\n",
        "    categorical_features = df.select_dtypes(include=['object', 'category']).columns\n",
        "    numerical_features = df.select_dtypes(include=np.number).columns\n",
        "\n",
        "    # Drop any ground truth label if it exists\n",
        "    if 'ground_truth_anomaly' in numerical_features:\n",
        "        numerical_features = numerical_features.drop('ground_truth_anomaly')\n",
        "\n",
        "    # Create preprocessing pipelines for both feature types\n",
        "    numerical_transformer = StandardScaler()\n",
        "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "    # Create a column transformer to apply different transformations to different columns\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ])\n",
        "\n",
        "    return preprocessor, numerical_features, categorical_features\n",
        "\n",
        "# --- Main App UI ---\n",
        "st.title(\"üîé Public Procurement Anomaly Detector\")\n",
        "st.write(\"\"\"\n",
        "This application uses the **Isolation Forest** algorithm to identify potential anomalies in procurement data.\n",
        "Upload your dataset or use the sample data to begin. Adjust the model parameters in the sidebar and view the\n",
        "detected anomalies and their explanations powered by **SHAP**.\n",
        "\"\"\")\n",
        "\n",
        "# --- Sidebar for Controls ---\n",
        "with st.sidebar:\n",
        "    st.header(\"‚öôÔ∏è Controls\")\n",
        "\n",
        "    st.subheader(\"1. Data Input\")\n",
        "    uploaded_file = st.file_uploader(\"Upload your CSV file\", type=[\"csv\"])\n",
        "    use_sample_data = st.button(\"Use Sample Data\")\n",
        "\n",
        "    st.subheader(\"2. Model Parameters\")\n",
        "    contamination = st.slider(\n",
        "        \"Contamination Level\",\n",
        "        min_value=0.01, max_value=0.5, value=0.05, step=0.01,\n",
        "        help=\"The expected proportion of anomalies in the data. This is a key parameter for Isolation Forest.\"\n",
        "    )\n",
        "    n_estimators = st.slider(\n",
        "        \"Number of Estimators\",\n",
        "        min_value=50, max_value=500, value=100, step=10,\n",
        "        help=\"The number of base trees in the ensemble.\"\n",
        "    )\n",
        "    random_seed = st.number_input(\n",
        "        \"Random Seed\",\n",
        "        value=42,\n",
        "        help=\"Seed for reproducibility. Change this to see how it affects the model's outcome.\"\n",
        "    )\n",
        "\n",
        "# --- Data Loading and Caching ---\n",
        "@st.cache_data\n",
        "def load_data(file):\n",
        "    return pd.read_csv(file)\n",
        "\n",
        "if use_sample_data:\n",
        "    df = generate_synthetic_data()\n",
        "    st.session_state['df'] = df\n",
        "elif uploaded_file is not None:\n",
        "    df = load_data(uploaded_file)\n",
        "    st.session_state['df'] = df\n",
        "else:\n",
        "    df = None\n",
        "\n",
        "if 'df' in st.session_state and st.session_state['df'] is not None:\n",
        "    df = st.session_state['df']\n",
        "\n",
        "    st.header(\"üìä Data Preview\")\n",
        "    st.dataframe(df.head())\n",
        "\n",
        "    # --- Preprocessing and Model Training ---\n",
        "    st.header(\"üöÄ Analysis Results\")\n",
        "    try:\n",
        "        # Define features to be used in the model\n",
        "        if 'ground_truth_anomaly' in df.columns:\n",
        "            features_df = df.drop('ground_truth_anomaly', axis=1)\n",
        "        else:\n",
        "            features_df = df.copy()\n",
        "\n",
        "        preprocessor, num_features, cat_features = preprocess_data(features_df)\n",
        "\n",
        "        # Create the model pipeline\n",
        "        model = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('isolator', IsolationForest(\n",
        "                n_estimators=n_estimators,\n",
        "                contamination=contamination,\n",
        "                random_state=random_seed,\n",
        "                n_jobs=-1\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        with st.spinner(\"Analyzing data and training model...\"):\n",
        "            # Fit the model and get predictions\n",
        "            model.fit(features_df)\n",
        "            df['anomaly_score'] = model.named_steps['isolator'].decision_function(model.named_steps['preprocessor'].transform(features_df))\n",
        "            df['predicted_anomaly'] = model.named_steps['isolator'].predict(model.named_steps['preprocessor'].transform(features_df))\n",
        "            # Convert predictions from -1/1 to 1/0\n",
        "            df['predicted_anomaly'] = df['predicted_anomaly'].apply(lambda x: 1 if x == -1 else 0)\n",
        "\n",
        "\n",
        "        anomalies = df[df['predicted_anomaly'] == 1].sort_values(by='anomaly_score', ascending=True)\n",
        "\n",
        "        st.subheader(f\"üö® Detected Anomalies ({len(anomalies)} found)\")\n",
        "        if not anomalies.empty:\n",
        "            st.dataframe(anomalies)\n",
        "        else:\n",
        "            st.success(\"No anomalies were detected with the current settings.\")\n",
        "\n",
        "        # --- Model Evaluation ---\n",
        "        if 'ground_truth_anomaly' in df.columns:\n",
        "            st.subheader(\"üìà Model Performance Metrics\")\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            precision = precision_score(df['ground_truth_anomaly'], df['predicted_anomaly'])\n",
        "            recall = recall_score(df['ground_truth_anomaly'], df['predicted_anomaly'])\n",
        "            f1 = f1_score(df['ground_truth_anomaly'], df['predicted_anomaly'])\n",
        "\n",
        "            col1.metric(\"Precision\", f\"{precision:.2%}\")\n",
        "            col2.metric(\"Recall\", f\"{recall:.2%}\")\n",
        "            col3.metric(\"F1-Score\", f\"{f1:.2%}\")\n",
        "            st.info(\"These metrics are calculated because the sample data includes a 'ground_truth_anomaly' column for validation. If you use your own data, this section will not appear unless a column with this exact name exists.\")\n",
        "\n",
        "\n",
        "        # --- SHAP Validation and Explanation ---\n",
        "        if not anomalies.empty:\n",
        "            st.header(\"üîç Anomaly Explanation with SHAP\")\n",
        "            st.write(\"\"\"\n",
        "            SHAP (SHapley Additive exPlanations) helps explain *why* a data point was flagged as an anomaly.\n",
        "            Features that push the score higher (to the right) contribute to the data point being considered 'normal',\n",
        "            while features that push the score lower (to the left) contribute to it being an 'anomaly'.\n",
        "            \"\"\")\n",
        "\n",
        "            with st.spinner(\"Calculating SHAP values... This may take a moment.\"):\n",
        "                # We need to use the transformed data for SHAP\n",
        "                data_transformed = model.named_steps['preprocessor'].transform(features_df)\n",
        "                transformed_feature_names = list(num_features) + \\\n",
        "                    model.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(cat_features).tolist()\n",
        "\n",
        "                # SHAP works with models that have a `predict` function.\n",
        "                # The decision_function provides a more nuanced score, so we wrap it.\n",
        "                def decision_function_wrapper(X):\n",
        "                    return model.named_steps['isolator'].decision_function(X)\n",
        "\n",
        "                explainer = shap.KernelExplainer(decision_function_wrapper, data_transformed, link=\"identity\")\n",
        "\n",
        "                # Use a smaller background set for faster computation if data is large\n",
        "                background_data = shap.sample(data_transformed, 100) if data_transformed.shape[0] > 100 else data_transformed\n",
        "                shap_explainer = shap.KernelExplainer(decision_function_wrapper, background_data)\n",
        "\n",
        "                # Get SHAP values for the anomalous data points\n",
        "                anomalies_transformed = model.named_steps['preprocessor'].transform(anomalies[features_df.columns])\n",
        "                shap_values = shap_explainer.shap_values(anomalies_transformed)\n",
        "\n",
        "            st.subheader(\"Summary of Anomaly Features\")\n",
        "            fig_summary, ax_summary = plt.subplots()\n",
        "            shap.summary_plot(shap_values, anomalies_transformed, feature_names=transformed_feature_names, show=False)\n",
        "            st.pyplot(fig_summary)\n",
        "            plt.close(fig_summary)\n",
        "            st.write(\"The summary plot above shows the most important features driving anomaly scores across all detected anomalies. For example, a high `total_cost` (red dot on the right of the `total_cost` row) strongly contributes to a record being flagged as an anomaly.\")\n",
        "\n",
        "            st.subheader(\"Individual Anomaly Explanations\")\n",
        "            # Select an anomaly to inspect\n",
        "            anomaly_to_inspect_index = st.selectbox(\n",
        "                \"Select an anomaly to inspect in detail:\",\n",
        "                options=anomalies.index,\n",
        "                format_func=lambda x: f\"Index {x} (Score: {anomalies.loc[x, 'anomaly_score']:.2f})\"\n",
        "            )\n",
        "\n",
        "            if anomaly_to_inspect_index:\n",
        "                anomaly_idx_in_anomalies_df = anomalies.index.get_loc(anomaly_to_inspect_index)\n",
        "\n",
        "                # Create a force plot for the selected anomaly\n",
        "                fig_force, ax_force = plt.subplots(figsize=(10, 3))\n",
        "                shap.force_plot(\n",
        "                    shap_explainer.expected_value,\n",
        "                    shap_values[anomaly_idx_in_anomalies_df, :],\n",
        "                    anomalies_transformed[anomaly_idx_in_anomalies_df, :],\n",
        "                    feature_names=transformed_feature_names,\n",
        "                    matplotlib=True,\n",
        "                    show=False\n",
        "                )\n",
        "                plt.tight_layout()\n",
        "                st.pyplot(fig_force)\n",
        "                plt.close(fig_force)\n",
        "\n",
        "                st.write(\"**Original Data for this Anomaly:**\")\n",
        "                st.dataframe(pd.DataFrame(anomalies.loc[anomaly_to_inspect_index]).T)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred during analysis: {e}\")\n",
        "        st.exception(e)\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload a file or use the sample data to get started.\")\n",
        "\n",
        "# --- Footer ---\n",
        "st.markdown(\"---\")\n",
        "st.write(\"Built with ‚ù§Ô∏è using Streamlit, Scikit-learn, and SHAP.\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'streamlit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3229256484>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIsolationForest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "WgFrHVEMIpXq",
        "outputId": "138352a1-7295-458b-ec38-f584a9626996"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}